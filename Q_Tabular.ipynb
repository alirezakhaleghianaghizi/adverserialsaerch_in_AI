{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alirezakhaleghianaghizi/adverserialsaerch_in_AI/blob/master/Q_Tabular.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYcz8dlDoP13"
      },
      "source": [
        "<img src='http://www-scf.usc.edu/~ghasemig/images/sharif.png' alt=\"SUT logo\" width=300 height=300 align=left class=\"saturate\" >\n",
        "\n",
        "<br>\n",
        "<font>\n",
        "<div dir=ltr align=center>\n",
        "<font color=0F5298 size=7>\n",
        "    Artificial Intelligence <br>\n",
        "<font color=2565AE size=5>\n",
        "    Computer Engineering Department <br>\n",
        "    Spring 2023<br>\n",
        "<font color=3C99D size=5>\n",
        "    Practical Assignment 3 - Reinforcement Learning <br>\n",
        "<font color=696880 size=4>\n",
        "    Mohammad Moshtaghi - Ali Salesi - Hossein Goli\n",
        "\n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTvyx2ksoP15"
      },
      "source": [
        "# Personal Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bnZBb2rZdGl"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ejuUsErSoP16"
      },
      "outputs": [],
      "source": [
        "# Set your student number\n",
        "student_number = '99101462'\n",
        "first_name = 'alireza'\n",
        "last_name = 'khaleghi'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "jvkpwce-oP17"
      },
      "outputs": [],
      "source": [
        "!pip install gym[toy_text]\n",
        "\n",
        "import gym \n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm import trange\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "clear_output() # You can use this method to clear your cell's output. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scirAV6ZoP17"
      },
      "source": [
        "# Q1: Q-Learning (100 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fnVbCZboP17"
      },
      "source": [
        "<font size=4>\n",
        "Author: Mohammad Moshtaghi\n",
        "<br/>\n",
        "<font color=red>\n",
        "Please run all the cells.\n",
        "</font>\n",
        "</font>\n",
        "<br/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMEt0G2loP18"
      },
      "source": [
        "# 1. Cliff Walking (70 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN5Q6YQGoP18"
      },
      "source": [
        "In this section we are going to implement different Temporal Difference algorithms and compare their results. We start with a simple problem called **Cliff Walking**. You may have seen this game in your lecture slides and here we are going to train an RL Agent to play this game optimally.\n",
        "\n",
        "First, lets get familiar with game's environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liqkzFf0oP18"
      },
      "source": [
        "## 1-1. Environment (10 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODAF8tq-oP18"
      },
      "source": [
        "Lets declare our environment and see some of its hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7I4dvTjQoP18",
        "outputId": "a18cf940-38a2-4348-bdf1-ea2db7c20f68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(4)\n",
            "Observation Space: Discrete(48)\n",
            "Max Episode Steps: None\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: None\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('CliffWalking-v0')\n",
        "spec = gym.spec('CliffWalking-v0')\n",
        "\n",
        "print(f\"Action Space: {env.action_space}\")\n",
        "print(f\"Observation Space: {env.observation_space}\")\n",
        "print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "print(f\"Reward Range: {env.reward_range}\")\n",
        "print(f\"Reward Threshold: {spec.reward_threshold}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "it7--eegoP18"
      },
      "outputs": [],
      "source": [
        "Actions =  {0: 'UP', \n",
        "            1: 'RIGHT',\n",
        "            2: 'DOWN',\n",
        "            3: 'LEFT'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A3kng05oP19"
      },
      "source": [
        "You can use **_visualize_** function to draw your state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "2ioGNeoHoP19"
      },
      "outputs": [],
      "source": [
        "def visualize(env, action=None, reward=None):\n",
        "    env_screen = env.render(mode = 'rgb_array')\n",
        "    plt.imshow(env_screen)\n",
        "    plt.axis('off');\n",
        "    title = ''\n",
        "    if action:\n",
        "        title += f'Action: {Actions[action]}'\n",
        "    if reward:\n",
        "        title += f'Reward: {reward}'\n",
        "    \n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da5M6v4koP19"
      },
      "source": [
        "Test `visualize` function with a random action. First, using `env.reset` function, we reset our environment so that our agent returns to the starting point. for moving your agent, use `env.step` function. it returns four values:\n",
        "\n",
        "1. next_state\n",
        "2. reward\n",
        "3. done\n",
        "4. some info (Honestly, it doesn't matter)\n",
        "\n",
        "You may need this functions later :)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX5WZW5NoP19",
        "outputId": "84588aa8-8c3d-4dfc-e42e-b830ea9cbb85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n",
            "-1\n",
            "False\n",
            "47\n",
            "-1\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "import random \n",
        "env.reset() # You can use this function to reset your environment.\n",
        "######################## YOUR CODE HERE ########################\n",
        "\n",
        "######################## END YOUR CODE #########################\n",
        "\n",
        "state_p, reward, done,_=env.step(0)\n",
        "print(state_p)\n",
        "print(reward)\n",
        "print(done)\n",
        "state_p, reward, done,_=env.step(1)\n",
        "state_p, reward, done,_=env.step(1)\n",
        "state_p, reward, done,_=env.step(1)\n",
        "state_p, reward, done,_=env.step(1)\n",
        "state_p, reward, done,_=env.step(1)\n",
        "state_p, reward, done,_=env.step(1)\n",
        "state_p, reward, done,_=env.step(1)\n",
        "state_p, reward, done,_=env.step(1)\n",
        "state_p, reward, done,_=env.step(1)\n",
        "state_p, reward, done,_=env.step(1)\n",
        "state_p, reward, done,_=env.step(1)\n",
        "state_p, reward, done,_=env.step(2)\n",
        "print(state_p)\n",
        "print(reward)\n",
        "print(done)\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WlLBoO2oP1-"
      },
      "source": [
        "## 1-2. Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DhxOgzpoP1-"
      },
      "source": [
        "Please read the class below. You must inherite this class in the following sections and implement different RL algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "4OCXsm5aoP1-"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "\n",
        "    def __init__(self, env, noise):\n",
        "        self.q_values = []\n",
        "        self.q_values= np.zeros(shape=(env.observation_space.n,env.action_space.n))\n",
        "        self.policy = {}\n",
        "        self.env = env\n",
        "        self.state=env.reset()\n",
        "        self.noise = noise\n",
        "        ######################## YOUR CODE HERE ########################\n",
        "        # Declare any variables you need.\n",
        "        ######################## END YOUR CODE #########################\n",
        "    def update_q(self, state, action, reward, next_state, alpha,gamma):\n",
        "        sample = reward + np.amax(self.q_values[next_state]) * gamma\n",
        "        self.q_values[state][action] = self.q_values[state][action] * (1 - alpha) + alpha * sample\n",
        "\n",
        "    def get_action(self, state, epsilon):\n",
        "      p = np.random.random()\n",
        "      if p > epsilon:\n",
        "          return np.argmax(self.q_values[state])\n",
        "      else:\n",
        "          return np.random.choice(range(env.action_space.n))\n",
        "\n",
        "    def learn(self, num_episodes, alpha, gamma, epsilon):\n",
        "        \"\"\"\n",
        "        Implement your Reinforcement Learning algorithm and train your agent in this function. \n",
        "        At the end, you must fill the q_values array.\n",
        "\n",
        "        Inputs:\n",
        "            - alpha: Learning rate\n",
        "            - gamma: Discount factor\n",
        "            - epsilon: The probability that the agent will act randomly instead of greedy in sampling.\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"your code here\"\"\"\n",
        "        for episode in range(num_episodes):\n",
        "           state, score, done, step =(self.env.reset()), 0, False, 0\n",
        "           while not done:\n",
        "              action = self.get_action(state, epsilon=epsilon)\n",
        "              next_state, reward, done, _ = self.env.step(action)\n",
        "              self.update_q(state, action, reward, next_state ,alpha,gamma)\n",
        "              state = next_state              \n",
        "        self.env.reset()\n",
        "\n",
        "    def create_policy(self):\n",
        "        \"\"\"\n",
        "        Create your policy in this function after your agent learns the q_values.\n",
        "        \"\"\"\n",
        "        ######################## YOUR CODE HERE ########################\n",
        "\n",
        "        ######################## END YOUR CODE #########################\n",
        "        print(\"ok\")\n",
        "        for states in range(self.env.observation_space.n):\n",
        "          \n",
        "          qvalstat=list(self.q_values[states])\n",
        "          max1=max(qvalstat)\n",
        "          #print(self.q_values[states],max1,states)\n",
        "          self.policy[states]=list(qvalstat).index(max1)\n",
        "          #print(max1,'max',list(qvalstat).index(max1),'action')\n",
        "          \n",
        "        \n",
        "    def act(self):\n",
        "        \"\"\"\n",
        "        Move your agent one step according to your policy.\n",
        "        \"\"\"\n",
        "        ######################## YOUR CODE HERE ########################\n",
        "\n",
        "        ######################## END YOUR CODE #########################\n",
        "        action=self.policy[self.state]\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "        self.update_q(self.state,action,reward,next_state,alpha,gamma)\n",
        "        self.state=next_state      \n",
        "        return [reward,done]\n",
        "\n",
        "\n",
        "    def evaluate(self, num_episodes):\n",
        "        \"\"\"\n",
        "        Sample num_episodes episodes from your agent that acts according to your policy.\n",
        "        Then return the average rewards it gets.\n",
        "\n",
        "        Inputs:\n",
        "            - num_episodes: Number of episodes for sampling.\n",
        "        \"\"\"\n",
        "        ######################## YOUR CODE HERE ########################\n",
        "\n",
        "        ######################## END YOUR CODE #########################\n",
        "        scores =[]\n",
        "        steps=[]\n",
        "        for episode in range(num_episodes):\n",
        "          self.state, score, done, step = (self.env.reset()), 0, False, 0\n",
        "          while not done:\n",
        "            action = self.policy[self.state]\n",
        "            self.state, reward, done, _ = self.env.step(action)\n",
        "            step += 1\n",
        "            score += int(reward)\n",
        "          steps.append(step)\n",
        "          scores.append(score)\n",
        "        print(\"Average score over 1000 run : \", np.array(scores).mean())\n",
        "        return [scores,steps]\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZQ2TvLcoP1-"
      },
      "source": [
        "## 1-3. Q-Learning (15 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suxVgod6oP1_"
      },
      "source": [
        "In this section, you must use **_Q-Learning_** algorithm to train your agent. Note that the action-value function Q(s,a) is updated iteratively as follows:\n",
        "\n",
        "$$ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\right) $$\n",
        "\n",
        "where ‍‍‍‍‍`r` is the reward received after taking action a in state `s`, `s'` is the new state, $\\gamma$ is the discount factor (a value between 0 and 1 that determines the importance of future rewards), and $\\alpha$ is the learning rate (determines the step size at which the value function is updated)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5-OtwfaoP1_"
      },
      "outputs": [],
      "source": [
        "class CliffWalkerQL(Agent):\n",
        "    \n",
        "    def learn(self, num_episodes, alpha, gamma, epsilon):\n",
        "        ######################## YOUR CODE HERE ########################\n",
        "\n",
        "        ######################## END YOUR CODE #########################\n",
        "        for episode in range(num_episodes):\n",
        "           state, score, done, step =(self.env.reset()), 0, False, 0\n",
        "           while not done:\n",
        "              action = self.get_action(state, epsilon=epsilon)\n",
        "              next_state, reward, done, _ = self.env.step(action)\n",
        "              self.update_q(state, action, reward, next_state ,alpha,gamma)\n",
        "              state = next_state              \n",
        "        self.env.reset()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBKV2hJhoP1_"
      },
      "source": [
        "## 1-4. Q-Learning Evaluation (10 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFKezv_moP1_"
      },
      "source": [
        "Train your agent and then evaluate it and display the result. Using the `visualize` function, show the path your agent takes in one of the episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkV1SG7moP1_"
      },
      "outputs": [],
      "source": [
        "cliff_walker_ql = CliffWalkerQL(env, 0)\n",
        "alpha = 0.8\n",
        "gamma = 0.95\n",
        "epsilon = 0.2\n",
        "episodes = 1000\n",
        "cliff_walker_ql.learn(episodes, alpha, gamma, epsilon)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEiho8PGoP2A"
      },
      "outputs": [],
      "source": [
        "######################## YOUR CODE HERE ########################\n",
        "\n",
        "######################## END YOUR CODE #########################\n",
        "cliff_walker_ql.create_policy()\n",
        "state, score, done, step =(cliff_walker_ql.env.reset()), 0, False, 0\n",
        "cliff_walker_ql.create_policy\n",
        "while not done:\n",
        "  alpha = 0.8\n",
        "  gamma = 0.95\n",
        "  epsilon = 0.2\n",
        "  episodes = 1000\n",
        "  action=cliff_walker_ql.policy[state]\n",
        "  next_state, reward, done, _ = cliff_walker_ql.env.step(action)\n",
        "  visualize(cliff_walker_ql.env, action, reward)\n",
        "  cliff_walker_ql.update_q(state, action, reward, next_state ,alpha,gamma)\n",
        "  qvalstat=list(cliff_walker_ql.q_values[state])\n",
        "  max1=max(qvalstat)\n",
        "  cliff_walker_ql.policy[state]=list(qvalstat).index(max1)\n",
        "  state = next_state      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkurMyh-oP2A"
      },
      "source": [
        "## 1-5. SARSA (15 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhjKBZYCoP2A"
      },
      "source": [
        "This time, you should use the SARSA algorithm, which is slightly different from the Q-Learning in implementation. But the result may significantly differ, and you should explain this difference, if any. \n",
        "\n",
        "Note that the SARSA update rule can be represented as:\n",
        "\n",
        "$$Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha \\left( R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}) - Q(S_t,A_t) \\right)$$\n",
        "\n",
        "\n",
        "Where $Q(S_t,A_t)$ is the current estimate of the expected return for taking action $A_t$ in state $S_t$, $\\alpha$ is the learning rate, $R_{t+1}$ is the reward received after taking action $A_t$ in state $S_t$, $\\gamma$ is the discount factor, and $Q(S_{t+1},A_{t+1})$ is the estimated return for taking action $A_{t+1}$ in the next state $S_{t+1}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj9-ScmNoP2A"
      },
      "outputs": [],
      "source": [
        "class CliffWalkerSARSA(Agent):\n",
        "    \n",
        "    def update_sarsaq(self, state, action1,action2, reward, next_state, alpha,gamma):\n",
        "      sample = reward + (self.q_values[next_state][action2]) * gamma\n",
        "      self.q_values[state][action1] = self.q_values[state][action1] * (1 - alpha) + alpha * sample\n",
        "\n",
        "    def learn(self, num_episodes, alpha, gamma, epsilon):\n",
        "        ######################## YOUR CODE HERE ########################\n",
        "\n",
        "        ######################## END YOUR CODE #########################\n",
        "        for episode in range(num_episodes):\n",
        "           state, score, done, step =(self.env.reset()), 0, False, 0\n",
        "           action1 = self.get_action(state, epsilon=epsilon)\n",
        "           while not done:\n",
        "              next_state, reward, done, _ = self.env.step(action1)\n",
        "              action2 = self.get_action(next_state, epsilon=epsilon)\n",
        "              self.update_sarsaq(state, action1,action2, reward, next_state ,alpha,gamma)\n",
        "              state = next_state \n",
        "              action1 = action2              \n",
        "        self.env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3ELN8ykoP2A"
      },
      "source": [
        "## 1-6. SARSA Evaluation (10 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tvHz0QioP2B"
      },
      "source": [
        "Train your agent and then evaluate it and display the result. Using the `visualize` function, show the path your agent takes in one of the episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48kfDieBoP2B"
      },
      "outputs": [],
      "source": [
        "cliff_walker_sarsa = CliffWalkerSARSA(env, 0)\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.5\n",
        "episodes = 1000\n",
        "cliff_walker_sarsa.learn(episodes, alpha, gamma, epsilon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlsbhcH2oP2B"
      },
      "outputs": [],
      "source": [
        "######################## YOUR CODE HERE ########################\n",
        "\n",
        "######################## END YOUR CODE #########################\n",
        "cliff_walker_sarsa.create_policy()\n",
        "state, score, done, step =(cliff_walker_sarsa.env.reset()), 0, False, 0\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.5\n",
        "episodes = 1000\n",
        "while not done:\n",
        "  action1=cliff_walker_sarsa.policy[state]\n",
        "  next_state, reward, done, _ = cliff_walker_sarsa.env.step(action1)\n",
        "  action2=cliff_walker_sarsa.policy[next_state]\n",
        "  cliff_walker_sarsa.update_sarsaq(state, action1,action2, reward*3, next_state ,alpha,gamma)\n",
        "  qvalstat=list(cliff_walker_sarsa.q_values[state])\n",
        "  max1=max(qvalstat)\n",
        "  cliff_walker_sarsa.policy[state]=list(qvalstat).index(max1)\n",
        "  visualize(cliff_walker_sarsa.env, action1, reward)\n",
        "  state = next_state  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di_hySjToP2B"
      },
      "source": [
        "## 1-7. Increase Noises (10 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gEnW7wkoP2B"
      },
      "source": [
        "Increase noise and see how your obtained policy changes. Do this for both above algorithms and repeat above steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gb65S4LzIIom"
      },
      "outputs": [],
      "source": [
        "cliff_walker_ql = CliffWalkerQL(env, 0)\n",
        "alpha = 0.8\n",
        "gamma = 0.95\n",
        "epsilon = 0.2\n",
        "episodes = 1000\n",
        "cliff_walker_ql.learn(episodes, alpha, gamma, epsilon)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiYXVq-GIIon"
      },
      "outputs": [],
      "source": [
        "######################## YOUR CODE HERE ########################\n",
        "\n",
        "######################## END YOUR CODE #########################   noiz with prob 0.3 for q learn\n",
        "cliff_walker_ql.create_policy()\n",
        "state, score, done, step =(cliff_walker_ql.env.reset()), 0, False, 0\n",
        "cliff_walker_ql.create_policy\n",
        "probnoiz=0.3\n",
        "while not done:\n",
        "  action=cliff_walker_ql.policy[state]\n",
        "  p = np.random.random()\n",
        "  if p < probnoiz:\n",
        "      action=np.random.choice(range(env.action_space.n))\n",
        "  alpha = 0.8\n",
        "  gamma = 0.95\n",
        "  epsilon = 0.2\n",
        "  episodes = 1000\n",
        "\n",
        "  next_state, reward, done, _ = cliff_walker_ql.env.step(action)\n",
        "  visualize(cliff_walker_ql.env, action, reward)\n",
        "  cliff_walker_ql.update_q(state, action, reward, next_state ,alpha,gamma)\n",
        "  qvalstat=list(cliff_walker_ql.q_values[state])\n",
        "  max1=max(qvalstat)\n",
        "  cliff_walker_ql.policy[state]=list(qvalstat).index(max1)\n",
        "  state = next_state      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r9B41MfIySP",
        "outputId": "be19e777-eb69-4b8a-f414-20a9571089e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "cliff_walker_sarsa = CliffWalkerSARSA(env, 0)\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.5\n",
        "episodes = 1000\n",
        "cliff_walker_sarsa.learn(episodes, alpha, gamma, epsilon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZHfxam0IySV"
      },
      "outputs": [],
      "source": [
        "######################## YOUR CODE HERE ########################\n",
        "\n",
        "######################## END YOUR CODE #########################\n",
        "cliff_walker_sarsa.create_policy()\n",
        "state, score, done, step =(cliff_walker_sarsa.env.reset()), 0, False, 0\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "epsilon = 0.5\n",
        "episodes = 1000\n",
        "probnoiz=0.3\n",
        "while not done:\n",
        "  action1=cliff_walker_sarsa.policy[state]\n",
        "  p = np.random.random()\n",
        "  if p < probnoiz:\n",
        "      action1=np.random.choice(range(env.action_space.n))\n",
        "  next_state, reward, done, _ = cliff_walker_sarsa.env.step(action1)\n",
        "  action2=cliff_walker_sarsa.policy[next_state]\n",
        "  p = np.random.random()\n",
        "  if p < probnoiz:\n",
        "      action2=np.random.choice(range(env.action_space.n))\n",
        "  cliff_walker_sarsa.update_sarsaq(state, action1,action2, reward*3, next_state ,alpha,gamma)\n",
        "  qvalstat=list(cliff_walker_sarsa.q_values[state])\n",
        "  max1=max(qvalstat)\n",
        "  cliff_walker_sarsa.policy[state]=list(qvalstat).index(max1)\n",
        "  visualize(cliff_walker_sarsa.env, action1, reward)\n",
        "  state = next_state  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEg5nVnXoP2B"
      },
      "source": [
        "# 2. Taxi Driver (30 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8GWlB-6oP2C"
      },
      "source": [
        "The next game that we want to implement is **Taxi Driver**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBewYB9moP2C"
      },
      "source": [
        "## 2-1. Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "TnD4_K8FoP2C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d54b56-4dd7-4513-f5ae-5cd21f390088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(6)\n",
            "Observation Space: Discrete(500)\n",
            "Max Episode Steps: 200\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: 8\n",
            "\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('Taxi-v3')\n",
        "spec = gym.spec('Taxi-v3')\n",
        "\n",
        "print(f\"Action Space: {env.action_space}\")\n",
        "print(f\"Observation Space: {env.observation_space}\")\n",
        "print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "print(f\"Reward Range: {env.reward_range}\")\n",
        "print(f\"Reward Threshold: {spec.reward_threshold}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "4AhdGwuooP2C"
      },
      "outputs": [],
      "source": [
        "Actions =  {0: 'DOWN', \n",
        "            1: 'UP',\n",
        "            2: 'RIGHT',\n",
        "            3: 'LEFT',\n",
        "            4: 'Pickup passenger',\n",
        "            5: 'Drop off passenger'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF-XtfKioP2C"
      },
      "source": [
        "You can read more about the game and its observation space in this [link](https://gymnasium.farama.org/environments/toy_text/taxi/#observation-space).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMwyELLJoP2C"
      },
      "source": [
        "## 2-2. Q-Learning (5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbdPELz8oP2C"
      },
      "source": [
        "Implement Q-Learning algorithm for this problem. (Of course, you can use the code you implemented in the previous section and just enjoy the result (: )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "maZzviMUoP2D"
      },
      "outputs": [],
      "source": [
        "class TaxiQL(Agent):\n",
        "    def learn(self, num_episodes, alpha, gamma, epsilon):\n",
        "        ######################## YOUR CODE HERE ########################\n",
        "\n",
        "        ######################## END YOUR CODE #########################\n",
        "        for episode in range(num_episodes):\n",
        "           state, score, done, step =(self.env.reset()), 0, False, 0\n",
        "           while not done:\n",
        "              action = self.get_action(state, epsilon=epsilon)\n",
        "              next_state, reward, done, _ = self.env.step(action)\n",
        "              self.update_q(state, action, reward, next_state ,alpha,gamma)\n",
        "              state = next_state              \n",
        "        self.env.reset()\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AukDQcvooP2D"
      },
      "source": [
        "## 2-3. Q-Learning Evaluation (5 pts) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbFl5JzboP2D"
      },
      "source": [
        "Train your agent two times, once with 1000 episodes and once with 10000 episodes. Then evaluate it and display the result. Using the `visualize` function, show the path your agent takes in one of the episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "mcv5hs-WoP2D"
      },
      "outputs": [],
      "source": [
        "taxi_ql = TaxiQL(env, 0)\n",
        "alpha = 0.8\n",
        "gamma = 0.95\n",
        "epsilon = 0.2\n",
        "episodes = 1000\n",
        "taxi_ql.learn(episodes, alpha, gamma, epsilon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GTyX29QoP2D"
      },
      "outputs": [],
      "source": [
        "######################## YOUR CODE HERE ########################\n",
        "print(taxi_ql.policy)\n",
        "taxi_ql.create_policy()\n",
        "state, score, done, step =(taxi_ql.env.reset()), 0, False, 0\n",
        "taxi_ql.create_policy\n",
        "while not done:\n",
        "  alpha = 0.8\n",
        "  gamma = 0.95\n",
        "  epsilon = 0.2\n",
        "  episodes = 1000\n",
        "  action=taxi_ql.policy[state]\n",
        "  next_state, reward, done, _ = taxi_ql.env.step(action)\n",
        "  visualize(taxi_ql.env, action, reward)\n",
        "  taxi_ql.update_q(state, action, reward, next_state ,alpha,gamma)\n",
        "  qvalstat=list(taxi_ql.q_values[state])\n",
        "  max1=max(qvalstat)\n",
        "  taxi_ql.policy[state]=list(qvalstat).index(max1)\n",
        "  state = next_state  \n",
        "######################## END YOUR CODE #########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "-2G4IGCXoP2E"
      },
      "outputs": [],
      "source": [
        "taxi_ql = TaxiQL(env, 0)\n",
        "alpha = 0.8\n",
        "gamma = 0.95\n",
        "epsilon = 0.2\n",
        "episodes = 10000\n",
        "taxi_ql.learn(episodes, alpha, gamma, epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98VAxqlxoP2E"
      },
      "outputs": [],
      "source": [
        "######################## YOUR CODE HERE ########################\n",
        "\n",
        "######################## END YOUR CODE #########################\n",
        "print(taxi_ql.policy)\n",
        "taxi_ql.create_policy()\n",
        "state, score, done, step =(taxi_ql.env.reset()), 0, False, 0\n",
        "taxi_ql.create_policy\n",
        "while not done:\n",
        "  alpha = 0.8\n",
        "  gamma = 0.95\n",
        "  epsilon = 0.2\n",
        "  episodes = 10000\n",
        "  action=taxi_ql.policy[state]\n",
        "  next_state, reward, done, _ = taxi_ql.env.step(action)\n",
        "  visualize(taxi_ql.env, action, reward)\n",
        "  taxi_ql.update_q(state, action, reward, next_state ,alpha,gamma)\n",
        "  qvalstat=list(taxi_ql.q_values[state])\n",
        "  max1=max(qvalstat)\n",
        "  taxi_ql.policy[state]=list(qvalstat).index(max1)\n",
        "  state = next_state  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO7vOkoLoP2E"
      },
      "source": [
        "## 2-4. TD(2) (15 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N07_6aV2oP2E"
      },
      "source": [
        "In this section, you have to implement the Taxi problem using TD(2) algorithm. The difference between this method and the previous methods is in the number of movements that we look from the future.\n",
        "\n",
        "For example, the SARSA TD(2) update rule can be represented as:\n",
        "\n",
        "$$Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha \\left( R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}) + \\gamma^2 Q(s_{t+2},A_{t+2}) - Q(S_t,A_t) \\right)$$\n",
        "\n",
        "\n",
        "Where $Q(S_t,A_t)$ is the current estimate of the expected return for taking action $A_t$ in state $S_t$, $\\alpha$ is the learning rate, $R_{t+1}$ is the reward received after taking action $A_t$ in state $S_t$, $\\gamma$ is the discount factor, and $Q(S_{t+1},A_{t+1})$ is the estimated return for taking action $A_{t+1}$ in the next state $S_{t+1}$.\n",
        "\n",
        "You can see [this video](https://youtu.be/AJiG3ykOxmY) for more details about TD methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "-NLyUfJBoP2E"
      },
      "outputs": [],
      "source": [
        "class TaxiTD2(Agent):\n",
        "    \n",
        "    def update_TD(self, state, action1,action2,action3, reward, next_state,next2_state, alpha,gamma):\n",
        "      sample = reward + (self.q_values[next_state][action2]) * gamma + (self.q_values[next2_state][action3]) * gamma* gamma\n",
        "      self.q_values[state][action1] = self.q_values[state][action1] * (1 - alpha) + alpha * sample\n",
        "\n",
        "    def learn(self, num_episodes, alpha, gamma, epsilon):\n",
        "        ######################## YOUR CODE HERE ########################\n",
        "\n",
        "        ######################## END YOUR CODE #########################\n",
        "        for episode in range(num_episodes):\n",
        "           state, score, done, step =(self.env.reset()), 0, False, 0\n",
        "           action1 = self.get_action(state, epsilon=epsilon)\n",
        "           next_state, reward1, done1, _ = self.env.step(action1)\n",
        "           action2 = self.get_action(next_state, epsilon=epsilon)\n",
        "           while not done:          \n",
        "              next2_state, reward2, done2, _ = self.env.step(action2)\n",
        "              action3 = self.get_action(next2_state, epsilon=epsilon)\n",
        "              self.update_TD(state, action1,action2,action3, reward1, next_state,next2_state ,alpha,gamma)\n",
        "              state = next_state \n",
        "              next_state = next2_state\n",
        "              action1 = action2 \n",
        "              action2 = action3\n",
        "              done=done1\n",
        "              done1=done2\n",
        "              reward1=reward2             \n",
        "        self.env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDsXkBX6oP2E"
      },
      "source": [
        "## 2-5. TD(2) Evaluation (5 pts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "4A7Og1YEoP2E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25027135-9bc6-458c-aa16-57130021bf53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-49-8e4acac99af2>:4: RuntimeWarning: overflow encountered in double_scalars\n",
            "  sample = reward + (self.q_values[next_state][action2]) * gamma + (self.q_values[next2_state][action3]) * gamma* gamma\n"
          ]
        }
      ],
      "source": [
        "taxi_td2 = TaxiTD2(env, 0)\n",
        "alpha = 0.8\n",
        "gamma = 0.95\n",
        "epsilon = 0.2\n",
        "episodes = 10000\n",
        "taxi_td2.learn(episodes, alpha, gamma, epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "taxi_td2.create_policy()\n",
        "print(taxi_td2.policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8isfESypRMLN",
        "outputId": "da44eee3-c259-4cc1-d078-a6157f66abe1"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ok\n",
            "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 4, 7: 0, 8: 0, 9: 0, 10: 0, 11: 2, 12: 0, 13: 0, 14: 0, 15: 0, 16: 5, 17: 0, 18: 0, 19: 0, 20: 0, 21: 3, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 1, 28: 3, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 1, 35: 0, 36: 0, 37: 0, 38: 3, 39: 3, 40: 0, 41: 0, 42: 2, 43: 2, 44: 0, 45: 0, 46: 0, 47: 2, 48: 2, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 3, 58: 3, 59: 0, 60: 0, 61: 3, 62: 0, 63: 1, 64: 2, 65: 0, 66: 0, 67: 0, 68: 0, 69: 3, 70: 0, 71: 3, 72: 0, 73: 2, 74: 0, 75: 0, 76: 3, 77: 2, 78: 0, 79: 0, 80: 0, 81: 3, 82: 3, 83: 0, 84: 0, 85: 0, 86: 3, 87: 0, 88: 3, 89: 0, 90: 0, 91: 0, 92: 3, 93: 0, 94: 0, 95: 0, 96: 3, 97: 5, 98: 3, 99: 0, 100: 0, 101: 0, 102: 0, 103: 0, 104: 0, 105: 0, 106: 2, 107: 0, 108: 0, 109: 2, 110: 0, 111: 0, 112: 0, 113: 0, 114: 0, 115: 0, 116: 2, 117: 0, 118: 0, 119: 0, 120: 0, 121: 0, 122: 0, 123: 0, 124: 0, 125: 0, 126: 0, 127: 3, 128: 0, 129: 0, 130: 0, 131: 0, 132: 0, 133: 0, 134: 0, 135: 0, 136: 0, 137: 0, 138: 0, 139: 0, 140: 0, 141: 0, 142: 0, 143: 0, 144: 0, 145: 0, 146: 0, 147: 2, 148: 0, 149: 0, 150: 0, 151: 0, 152: 0, 153: 0, 154: 2, 155: 0, 156: 0, 157: 0, 158: 0, 159: 0, 160: 0, 161: 0, 162: 0, 163: 0, 164: 2, 165: 0, 166: 0, 167: 0, 168: 0, 169: 1, 170: 0, 171: 0, 172: 0, 173: 0, 174: 0, 175: 0, 176: 3, 177: 2, 178: 0, 179: 0, 180: 0, 181: 3, 182: 3, 183: 3, 184: 0, 185: 0, 186: 0, 187: 0, 188: 0, 189: 0, 190: 0, 191: 0, 192: 0, 193: 3, 194: 0, 195: 0, 196: 3, 197: 0, 198: 1, 199: 0, 200: 0, 201: 0, 202: 0, 203: 0, 204: 0, 205: 0, 206: 1, 207: 2, 208: 0, 209: 0, 210: 0, 211: 0, 212: 2, 213: 2, 214: 2, 215: 0, 216: 2, 217: 0, 218: 0, 219: 0, 220: 0, 221: 0, 222: 0, 223: 0, 224: 2, 225: 0, 226: 0, 227: 2, 228: 0, 229: 0, 230: 0, 231: 0, 232: 0, 233: 0, 234: 0, 235: 0, 236: 0, 237: 2, 238: 0, 239: 0, 240: 0, 241: 2, 242: 0, 243: 0, 244: 1, 245: 0, 246: 0, 247: 2, 248: 2, 249: 0, 250: 0, 251: 0, 252: 0, 253: 0, 254: 0, 255: 0, 256: 0, 257: 2, 258: 3, 259: 0, 260: 0, 261: 0, 262: 0, 263: 0, 264: 0, 265: 0, 266: 0, 267: 0, 268: 0, 269: 1, 270: 0, 271: 0, 272: 0, 273: 0, 274: 0, 275: 0, 276: 0, 277: 0, 278: 0, 279: 0, 280: 0, 281: 1, 282: 1, 283: 0, 284: 0, 285: 0, 286: 0, 287: 1, 288: 0, 289: 3, 290: 0, 291: 0, 292: 3, 293: 0, 294: 0, 295: 0, 296: 0, 297: 3, 298: 1, 299: 0, 300: 0, 301: 0, 302: 0, 303: 0, 304: 1, 305: 0, 306: 1, 307: 1, 308: 0, 309: 0, 310: 0, 311: 0, 312: 1, 313: 0, 314: 1, 315: 0, 316: 0, 317: 0, 318: 0, 319: 0, 320: 0, 321: 0, 322: 1, 323: 0, 324: 2, 325: 0, 326: 0, 327: 1, 328: 1, 329: 0, 330: 0, 331: 0, 332: 0, 333: 0, 334: 0, 335: 0, 336: 0, 337: 1, 338: 0, 339: 0, 340: 0, 341: 1, 342: 0, 343: 0, 344: 1, 345: 0, 346: 0, 347: 1, 348: 0, 349: 0, 350: 0, 351: 0, 352: 0, 353: 0, 354: 0, 355: 0, 356: 0, 357: 3, 358: 1, 359: 0, 360: 0, 361: 0, 362: 0, 363: 0, 364: 0, 365: 0, 366: 0, 367: 0, 368: 0, 369: 1, 370: 0, 371: 0, 372: 0, 373: 0, 374: 0, 375: 0, 376: 0, 377: 0, 378: 0, 379: 0, 380: 0, 381: 0, 382: 0, 383: 0, 384: 0, 385: 0, 386: 0, 387: 0, 388: 0, 389: 1, 390: 0, 391: 0, 392: 0, 393: 0, 394: 0, 395: 0, 396: 0, 397: 0, 398: 0, 399: 0, 400: 0, 401: 0, 402: 0, 403: 0, 404: 1, 405: 0, 406: 5, 407: 1, 408: 0, 409: 0, 410: 0, 411: 0, 412: 1, 413: 0, 414: 1, 415: 0, 416: 0, 417: 0, 418: 0, 419: 0, 420: 0, 421: 0, 422: 0, 423: 0, 424: 1, 425: 0, 426: 0, 427: 1, 428: 1, 429: 0, 430: 0, 431: 0, 432: 0, 433: 0, 434: 0, 435: 0, 436: 0, 437: 1, 438: 0, 439: 0, 440: 0, 441: 0, 442: 0, 443: 0, 444: 1, 445: 0, 446: 0, 447: 3, 448: 0, 449: 0, 450: 0, 451: 0, 452: 0, 453: 0, 454: 0, 455: 0, 456: 0, 457: 0, 458: 0, 459: 0, 460: 0, 461: 0, 462: 0, 463: 0, 464: 0, 465: 0, 466: 0, 467: 0, 468: 0, 469: 1, 470: 0, 471: 0, 472: 0, 473: 0, 474: 0, 475: 0, 476: 0, 477: 0, 478: 0, 479: 0, 480: 0, 481: 0, 482: 0, 483: 0, 484: 0, 485: 0, 486: 0, 487: 0, 488: 0, 489: 1, 490: 0, 491: 0, 492: 0, 493: 0, 494: 0, 495: 0, 496: 0, 497: 0, 498: 0, 499: 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT72KaStoP2E"
      },
      "outputs": [],
      "source": [
        "######################## YOUR CODE HERE ########################\n",
        "\n",
        "######################## END YOUR CODE #########################\n",
        "taxi_td2.create_policy()\n",
        "state, score, done, step =(taxi_td2.env.reset()), 0, False, 0\n",
        "action1=taxi_td2.policy[state]\n",
        "next_state, reward1, done, _ = taxi_td2.env.step(action1)\n",
        "while not done:\n",
        "  alpha = 0.8\n",
        "  gamma = 0.95\n",
        "  epsilon = 0.2\n",
        "  episodes = 10000\n",
        "  action2=taxi_td2.policy[next_state]\n",
        "  next2_state, reward2, done1, _ = taxi_td2.env.step(action2)\n",
        "  action3=taxi_td2.policy[next2_state]\n",
        "  taxi_td2.update_TD(state, action1,action2,action3, reward*4, next_state,next2_state ,alpha,gamma)\n",
        "  qvalstat=list(taxi_td2.q_values[state])\n",
        "  max1=max(qvalstat)\n",
        "  taxi_td2.policy[state]=list(qvalstat).index(max1)\n",
        "  visualize(taxi_td2.env, action1, reward)\n",
        "  state = next_state \n",
        "  next_state=next2_state\n",
        "  done=done1\n",
        "  action1 = action2\n",
        "  reward1=reward2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onRgISesoP2F"
      },
      "source": [
        "You can see that the TD(2) method can reach an acceptable policy faster. Try to explain why."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "026f2bef8fb7be59296f2f39e2043bb013bc567dc5026fb77125b1034979614d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}